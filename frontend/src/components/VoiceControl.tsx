/**
 * VoiceControl Component
 * Handles voice input (Speech-to-Text) and voice output (Text-to-Speech)
 * 
 * Phase 1: Web Speech API (Browser-based)
 * Phase 2: Google Cloud Speech API (Production-ready)
 * 
 * Features:
 * - Arabic language support (ar-EG, ar-SA)
 * - Real-time speech recognition
 * - Natural voice synthesis
 * - Error handling and fallbacks
 * - Browser compatibility checks
 */

import React, { useState, useEffect, useRef, useCallback } from 'react';
import axios from 'axios';

// Type definitions for Web Speech API
interface SpeechRecognitionEvent extends Event {
  results: SpeechRecognitionResultList;
  resultIndex: number;
}

interface SpeechRecognitionErrorEvent extends Event {
  error: string;
  message: string;
}

interface SpeechRecognition extends EventTarget {
  continuous: boolean;
  interimResults: boolean;
  lang: string;
  maxAlternatives: number;
  start(): void;
  stop(): void;
  abort(): void;
  onstart: ((this: SpeechRecognition, ev: Event) => any) | null;
  onend: ((this: SpeechRecognition, ev: Event) => any) | null;
  onresult: ((this: SpeechRecognition, ev: SpeechRecognitionEvent) => any) | null;
  onerror: ((this: SpeechRecognition, ev: SpeechRecognitionErrorEvent) => any) | null;
}

declare global {
  interface Window {
    SpeechRecognition: new () => SpeechRecognition;
    webkitSpeechRecognition: new () => SpeechRecognition;
  }
}

interface VoiceControlProps {
  onTranscript?: (text: string) => void;
  onResponse?: (text: string) => void;
  language?: 'ar-EG' | 'ar-SA' | 'en-US';
  autoSpeak?: boolean;
  className?: string;
}

interface VoiceState {
  isListening: boolean;
  isSpeaking: boolean;
  transcript: string;
  interimTranscript: string;
  error: string | null;
  isSupported: boolean;
}

export const VoiceControl: React.FC<VoiceControlProps> = ({
  onTranscript,
  onResponse,
  language = 'ar-EG',
  autoSpeak = true,
  className = ''
}) => {
  // State management
  const [state, setState] = useState<VoiceState>({
    isListening: false,
    isSpeaking: false,
    transcript: '',
    interimTranscript: '',
    error: null,
    isSupported: false
  });

  // Refs
  const recognitionRef = useRef<SpeechRecognition | null>(null);
  const synthesisRef = useRef<SpeechSynthesis | null>(null);

  /**
   * Check browser support for Web Speech API
   */
  const checkSupport = useCallback(() => {
    const speechRecognitionSupported = 
      'SpeechRecognition' in window || 'webkitSpeechRecognition' in window;
    
    const speechSynthesisSupported = 'speechSynthesis' in window;

    const isSupported = speechRecognitionSupported && speechSynthesisSupported;

    setState(prev => ({ ...prev, isSupported }));

    if (!isSupported) {
      console.warn('Web Speech API not supported in this browser');
    }

    return isSupported;
  }, []);

  /**
   * Initialize Speech Recognition
   */
  const initializeSpeechRecognition = useCallback(() => {
    if (!checkSupport()) return null;

    const SpeechRecognitionAPI = 
      window.SpeechRecognition || window.webkitSpeechRecognition;

    const recognition = new SpeechRecognitionAPI();

    // Configuration
    recognition.continuous = false;        // Stop after one phrase
    recognition.interimResults = true;     // Get interim results
    recognition.lang = language;           // Set language
    recognition.maxAlternatives = 1;       // Get best match only

    // Event handlers
    recognition.onstart = () => {
      console.log('ğŸ¤ Voice recognition started');
      setState(prev => ({ 
        ...prev, 
        isListening: true, 
        error: null,
        transcript: '',
        interimTranscript: ''
      }));
    };

    recognition.onend = () => {
      console.log('ğŸ¤ Voice recognition ended');
      setState(prev => ({ ...prev, isListening: false }));
    };

    recognition.onresult = (event: SpeechRecognitionEvent) => {
      let interimTranscript = '';
      let finalTranscript = '';

      // Process all results
      for (let i = event.resultIndex; i < event.results.length; i++) {
        const transcript = event.results[i][0].transcript;
        
        if (event.results[i].isFinal) {
          finalTranscript += transcript;
        } else {
          interimTranscript += transcript;
        }
      }

      // Update state
      setState(prev => ({
        ...prev,
        transcript: finalTranscript || prev.transcript,
        interimTranscript
      }));

      // If we have a final transcript, process it
      if (finalTranscript) {
        console.log('ğŸ“ Final transcript:', finalTranscript);
        
        // Call callback
        if (onTranscript) {
          onTranscript(finalTranscript);
        }

        // Send to backend
        sendToBackend(finalTranscript);
      }
    };

    recognition.onerror = (event: SpeechRecognitionErrorEvent) => {
      console.error('ğŸ¤ Speech recognition error:', event.error);
      
      let errorMessage = 'Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØª';
      
      switch (event.error) {
        case 'no-speech':
          errorMessage = 'Ù„Ù… ÙŠØªÙ… Ø§ÙƒØªØ´Ø§Ù Ø£ÙŠ ØµÙˆØª. Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.';
          break;
        case 'audio-capture':
          errorMessage = 'Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†. ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø£Ø°ÙˆÙ†Ø§Øª.';
          break;
        case 'not-allowed':
          errorMessage = 'ØªÙ… Ø±ÙØ¶ Ø¥Ø°Ù† Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø³Ù…Ø§Ø­ Ø¨Ø§Ù„ÙˆØµÙˆÙ„.';
          break;
        case 'network':
          errorMessage = 'Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø´Ø¨ÙƒØ©. ØªØ­Ù‚Ù‚ Ù…Ù† Ø§ØªØµØ§Ù„ Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª.';
          break;
        case 'aborted':
          errorMessage = 'ØªÙ… Ø¥Ù„ØºØ§Ø¡ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØª.';
          break;
      }

      setState(prev => ({ 
        ...prev, 
        error: errorMessage,
        isListening: false 
      }));
    };

    return recognition;
  }, [language, onTranscript]);

  /**
   * Initialize Speech Synthesis
   */
  const initializeSpeechSynthesis = useCallback(() => {
    if ('speechSynthesis' in window) {
      synthesisRef.current = window.speechSynthesis;
      return true;
    }
    return false;
  }, []);

  /**
   * Start listening
   */
  const startListening = useCallback(() => {
    if (!state.isSupported) {
      setState(prev => ({ 
        ...prev, 
        error: 'Ø§Ù„Ù…ØªØµÙØ­ Ù„Ø§ ÙŠØ¯Ø¹Ù… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØª' 
      }));
      return;
    }

    if (state.isListening) {
      console.warn('Already listening');
      return;
    }

    try {
      if (!recognitionRef.current) {
        recognitionRef.current = initializeSpeechRecognition();
      }

      recognitionRef.current?.start();
    } catch (error) {
      console.error('Error starting recognition:', error);
      setState(prev => ({ 
        ...prev, 
        error: 'ÙØ´Ù„ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØª' 
      }));
    }
  }, [state.isSupported, state.isListening, initializeSpeechRecognition]);

  /**
   * Stop listening
   */
  const stopListening = useCallback(() => {
    if (recognitionRef.current && state.isListening) {
      recognitionRef.current.stop();
    }
  }, [state.isListening]);

  /**
   * Speak text using Text-to-Speech
   */
  const speak = useCallback((text: string) => {
    if (!synthesisRef.current) {
      console.error('Speech synthesis not available');
      return;
    }

    // Cancel any ongoing speech
    synthesisRef.current.cancel();

    const utterance = new SpeechSynthesisUtterance(text);
    utterance.lang = language;
    utterance.rate = 1.0;      // Normal speed
    utterance.pitch = 1.0;     // Normal pitch
    utterance.volume = 1.0;    // Full volume

    // Event handlers
    utterance.onstart = () => {
      console.log('ğŸ”Š Speaking started');
      setState(prev => ({ ...prev, isSpeaking: true }));
    };

    utterance.onend = () => {
      console.log('ğŸ”Š Speaking ended');
      setState(prev => ({ ...prev, isSpeaking: false }));
    };

    utterance.onerror = (event) => {
      console.error('ğŸ”Š Speech synthesis error:', event);
      setState(prev => ({ 
        ...prev, 
        isSpeaking: false,
        error: 'ÙØ´Ù„ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª'
      }));
    };

    // Speak
    synthesisRef.current.speak(utterance);
  }, [language]);

  /**
   * Stop speaking
   */
  const stopSpeaking = useCallback(() => {
    if (synthesisRef.current) {
      synthesisRef.current.cancel();
      setState(prev => ({ ...prev, isSpeaking: false }));
    }
  }, []);

  /**
   * Send transcript to backend
   */
  const sendToBackend = async (text: string) => {
    try {
      console.log('ğŸ“¤ Sending to backend:', text);

      const response = await axios.post('/api/ai/chat', {
        message: text,
        mode: 'voice',
        language: language.split('-')[0] // 'ar' or 'en'
      });

      if (response.data.success && response.data.response) {
        const aiResponse = response.data.response;
        console.log('ğŸ“¥ AI Response:', aiResponse);

        // Call callback
        if (onResponse) {
          onResponse(aiResponse);
        }

        // Auto-speak response if enabled
        if (autoSpeak) {
          speak(aiResponse);
        }
      }
    } catch (error) {
      console.error('Error sending to backend:', error);
      setState(prev => ({ 
        ...prev, 
        error: 'ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø®Ø§Ø¯Ù…' 
      }));
    }
  };

  /**
   * Initialize on mount
   */
  useEffect(() => {
    checkSupport();
    initializeSpeechSynthesis();

    // Cleanup on unmount
    return () => {
      if (recognitionRef.current) {
        recognitionRef.current.abort();
      }
      if (synthesisRef.current) {
        synthesisRef.current.cancel();
      }
    };
  }, [checkSupport, initializeSpeechSynthesis]);

  /**
   * Update language when prop changes
   */
  useEffect(() => {
    if (recognitionRef.current) {
      recognitionRef.current.lang = language;
    }
  }, [language]);

  // Render
  return (
    <div className={`voice-control ${className}`}>
      {/* Microphone Button */}
      <button
        onClick={state.isListening ? stopListening : startListening}
        disabled={!state.isSupported}
        className={`voice-button ${state.isListening ? 'listening' : ''} ${!state.isSupported ? 'disabled' : ''}`}
        title={state.isListening ? 'Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹' : 'Ø§Ø¨Ø¯Ø£ Ø§Ù„ØªØ­Ø¯Ø«'}
      >
        {state.isListening ? 'ğŸ¤ Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹...' : 'ğŸ¤ Ø§Ø¶ØºØ· Ù„Ù„ØªØ­Ø¯Ø«'}
      </button>

      {/* Speaker Button */}
      {state.isSpeaking && (
        <button
          onClick={stopSpeaking}
          className="voice-button speaking"
          title="Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ´ØºÙŠÙ„"
        >
          ğŸ”Š Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ´ØºÙŠÙ„...
        </button>
      )}

      {/* Transcript Display */}
      {(state.transcript || state.interimTranscript) && (
        <div className="transcript-display">
          {state.transcript && (
            <div className="final-transcript">
              <strong>Ø£Ù†Øª:</strong> {state.transcript}
            </div>
          )}
          {state.interimTranscript && (
            <div className="interim-transcript">
              <em>{state.interimTranscript}</em>
            </div>
          )}
        </div>
      )}

      {/* Error Display */}
      {state.error && (
        <div className="error-message">
          âš ï¸ {state.error}
        </div>
      )}

      {/* Browser Support Warning */}
      {!state.isSupported && (
        <div className="warning-message">
          âš ï¸ Ø§Ù„Ù…ØªØµÙØ­ Ø§Ù„Ø­Ø§Ù„ÙŠ Ù„Ø§ ÙŠØ¯Ø¹Ù… Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ØµÙˆØªÙŠØ©. 
          ÙŠØ±Ø¬Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù… Chrome Ø£Ùˆ Edge Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ ØªØ¬Ø±Ø¨Ø©.
        </div>
      )}
    </div>
  );
};

export default VoiceControl;
