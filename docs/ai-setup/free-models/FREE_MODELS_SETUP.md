# Free Models Setup Guide - Zero Cost AI Development

**Version:** 1.0  
**Last Updated:** October 15, 2025  
**Status:** üü¢ Active  
**Cost:** $0 - Completely Free!

---

## üéØ Overview

This guide sets up a **completely free** AI development environment using:
- ‚úÖ Local models (Ollama)
- ‚úÖ Free tier APIs
- ‚úÖ Open source models
- ‚úÖ No credit card required

**Total Cost:** $0/month

---

## üöÄ Quick Start (5 Minutes)

### Step 1: Install Ollama

**macOS/Linux:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

**Windows:**
```bash
# Download from: https://ollama.com/download
# Run installer
```

**Verify installation:**
```bash
ollama --version
```

### Step 2: Pull Free Models

```bash
# Autocomplete (Fast - 1.5B)
ollama pull qwen2.5-coder:1.5b

# Chat/Edit (Quality - 7B)
ollama pull qwen2.5-coder:7b

# Embeddings (For search)
ollama pull nomic-embed-text

# Optional: Better quality (14B)
ollama pull qwen2.5-coder:14b
```

### Step 3: Configure Continue

Create/update `~/.continue/config.json`:

```json
{
  "models": [
    {
      "title": "Qwen 7B Chat",
      "provider": "ollama",
      "model": "qwen2.5-coder:7b",
      "roles": ["chat", "edit"]
    },
    {
      "title": "Qwen 1.5B Autocomplete",
      "provider": "ollama",
      "model": "qwen2.5-coder:1.5b",
      "roles": ["autocomplete"]
    },
    {
      "title": "Nomic Embeddings",
      "provider": "ollama",
      "model": "nomic-embed-text",
      "roles": ["embed"]
    }
  ]
}
```

### Step 4: Test It!

1. Open VS Code
2. Open any code file
3. Start typing ‚Üí See autocomplete
4. Press `Cmd/Ctrl + L` ‚Üí Chat with AI
5. Select code + `Cmd/Ctrl + I` ‚Üí Edit with AI

**Done!** üéâ You have a free AI coding assistant!

---

## üìä Free Models Comparison

### Autocomplete Models

| Model | Size | Speed | Quality | RAM | Use Case |
|-------|------|-------|---------|-----|----------|
| **qwen2.5-coder:1.5b** | 1.5B | ‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | 4GB | Fast autocomplete |
| **qwen2.5-coder:3b** | 3B | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | 6GB | Balanced |
| **qwen2.5-coder:7b** | 7B | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 8GB | Best quality |

**Recommendation:** Start with 1.5B, upgrade to 7B if you have RAM.

---

### Chat/Edit Models

| Model | Size | Speed | Quality | RAM | Use Case |
|-------|------|-------|---------|-----|----------|
| **qwen2.5-coder:7b** | 7B | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | 8GB | General coding |
| **qwen2.5-coder:14b** | 14B | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 16GB | Complex tasks |
| **qwen2.5-coder:32b** | 32B | ‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 32GB | Maximum quality |
| **deepseek-coder:6.7b** | 6.7B | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | 8GB | Alternative |
| **codellama:7b** | 7B | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | 8GB | Meta's model |

**Recommendation:** qwen2.5-coder:7b for best balance.

---

### Embedding Models

| Model | Size | Speed | Quality | RAM | Use Case |
|-------|------|-------|---------|-----|----------|
| **nomic-embed-text** | 137M | ‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | 2GB | Code search |
| **mxbai-embed-large** | 335M | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 2GB | Better quality |
| **all-minilm** | 23M | ‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | 1GB | Ultra fast |

**Recommendation:** nomic-embed-text (best for code).

---

## üé® Complete Free Configuration

### Full Continue Config (All Free Models)

```json
{
  "models": [
    {
      "title": "Qwen 7B - Chat & Edit",
      "provider": "ollama",
      "model": "qwen2.5-coder:7b",
      "roles": ["chat", "edit"],
      "completionOptions": {
        "temperature": 0.2,
        "topP": 0.95,
        "numPredict": 2048,
        "numCtx": 8192
      }
    },
    {
      "title": "Qwen 1.5B - Autocomplete",
      "provider": "ollama",
      "model": "qwen2.5-coder:1.5b",
      "roles": ["autocomplete"],
      "completionOptions": {
        "temperature": 0.2,
        "topP": 0.95,
        "numPredict": 256,
        "numCtx": 2048
      }
    },
    {
      "title": "Nomic - Embeddings",
      "provider": "ollama",
      "model": "nomic-embed-text",
      "roles": ["embed"]
    }
  ],
  
  "tabAutocompleteModel": {
    "title": "Qwen 1.5B Autocomplete",
    "provider": "ollama",
    "model": "qwen2.5-coder:1.5b"
  },
  
  "embeddingsProvider": {
    "provider": "ollama",
    "model": "nomic-embed-text"
  }
}
```

---

## üîß Optimization Tips

### For Fast Autocomplete

```json
{
  "title": "Qwen 1.5B Fast",
  "provider": "ollama",
  "model": "qwen2.5-coder:1.5b",
  "roles": ["autocomplete"],
  "completionOptions": {
    "temperature": 0.1,
    "topP": 0.9,
    "numPredict": 128,
    "numCtx": 1024,
    "repeatPenalty": 1.1
  }
}
```

### For Better Quality

```json
{
  "title": "Qwen 7B Quality",
  "provider": "ollama",
  "model": "qwen2.5-coder:7b",
  "roles": ["chat", "edit"],
  "completionOptions": {
    "temperature": 0.3,
    "topP": 0.95,
    "numPredict": 4096,
    "numCtx": 16384,
    "repeatPenalty": 1.05
  }
}
```

### For Low RAM (4GB)

```json
{
  "models": [
    {
      "title": "Qwen 1.5B Only",
      "provider": "ollama",
      "model": "qwen2.5-coder:1.5b",
      "roles": ["chat", "edit", "autocomplete"],
      "completionOptions": {
        "numCtx": 2048
      }
    }
  ]
}
```

---

## üöÄ Performance Optimization

### Enable GPU Acceleration

**NVIDIA GPU:**
```bash
# Ollama automatically uses GPU if available
# Verify:
ollama run qwen2.5-coder:1.5b "test"
# Should show GPU usage in logs
```

**Apple Silicon (M1/M2/M3):**
```bash
# Automatically uses Metal
# Already optimized!
```

### Increase Context Window

```json
{
  "completionOptions": {
    "numCtx": 8192  // Default: 2048
  }
}
```

**Trade-off:** Larger context = slower but better understanding

### Adjust Temperature

```json
{
  "completionOptions": {
    "temperature": 0.1  // More deterministic (0.0-1.0)
  }
}
```

**Guide:**
- `0.0-0.2`: Very focused, deterministic
- `0.2-0.5`: Balanced (recommended)
- `0.5-1.0`: More creative, varied

---

## üì¶ Model Management

### List Installed Models

```bash
ollama list
```

### Remove Models (Free Space)

```bash
ollama rm qwen2.5-coder:32b
```

### Update Models

```bash
ollama pull qwen2.5-coder:7b
```

### Check Model Info

```bash
ollama show qwen2.5-coder:7b
```

---

## üéØ Use Cases & Recommendations

### Scenario 1: Limited RAM (4-8GB)

**Setup:**
```bash
ollama pull qwen2.5-coder:1.5b
```

**Config:**
```json
{
  "models": [
    {
      "title": "Qwen 1.5B All-in-One",
      "provider": "ollama",
      "model": "qwen2.5-coder:1.5b",
      "roles": ["chat", "edit", "autocomplete"]
    }
  ]
}
```

**Performance:** Fast, good quality for most tasks

---

### Scenario 2: Good RAM (8-16GB)

**Setup:**
```bash
ollama pull qwen2.5-coder:1.5b
ollama pull qwen2.5-coder:7b
ollama pull nomic-embed-text
```

**Config:**
```json
{
  "models": [
    {
      "title": "Qwen 7B Chat",
      "provider": "ollama",
      "model": "qwen2.5-coder:7b",
      "roles": ["chat", "edit"]
    },
    {
      "title": "Qwen 1.5B Autocomplete",
      "provider": "ollama",
      "model": "qwen2.5-coder:1.5b",
      "roles": ["autocomplete"]
    },
    {
      "title": "Nomic Embeddings",
      "provider": "ollama",
      "model": "nomic-embed-text",
      "roles": ["embed"]
    }
  ]
}
```

**Performance:** Excellent balance of speed and quality

---

### Scenario 3: High RAM (16GB+)

**Setup:**
```bash
ollama pull qwen2.5-coder:1.5b
ollama pull qwen2.5-coder:14b
ollama pull nomic-embed-text
```

**Config:**
```json
{
  "models": [
    {
      "title": "Qwen 14B Chat",
      "provider": "ollama",
      "model": "qwen2.5-coder:14b",
      "roles": ["chat", "edit"]
    },
    {
      "title": "Qwen 1.5B Autocomplete",
      "provider": "ollama",
      "model": "qwen2.5-coder:1.5b",
      "roles": ["autocomplete"]
    },
    {
      "title": "Nomic Embeddings",
      "provider": "ollama",
      "model": "nomic-embed-text",
      "roles": ["embed"]
    }
  ]
}
```

**Performance:** Maximum quality, still fast

---

## üîí Privacy Benefits

### 100% Local Processing

‚úÖ **All data stays on your machine**
- No code sent to external servers
- No API keys needed
- No usage tracking
- No rate limits
- No internet required (after download)

‚úÖ **Perfect for:**
- Sensitive code
- Proprietary projects
- Offline development
- Privacy-conscious developers
- Corporate environments

---

## üí∞ Cost Comparison

### Free (This Setup)

| Feature | Cost |
|---------|------|
| Autocomplete | $0 |
| Chat/Edit | $0 |
| Embeddings | $0 |
| API Calls | $0 |
| Rate Limits | None |
| **Total/Month** | **$0** |

### Paid Alternatives

| Service | Cost/Month |
|---------|------------|
| GitHub Copilot | $10-19 |
| Cursor Pro | $20 |
| Tabnine Pro | $12 |
| Codeium Pro | $10 |
| **Total** | **$52-61** |

**Savings:** $52-61/month = $624-732/year

---

## üêõ Troubleshooting

### Ollama Not Starting

**Problem:** `ollama: command not found`

**Solution:**
```bash
# macOS/Linux
curl -fsSL https://ollama.com/install.sh | sh

# Verify
which ollama
```

---

### Model Download Fails

**Problem:** Download interrupted or fails

**Solution:**
```bash
# Retry download
ollama pull qwen2.5-coder:7b

# Check disk space
df -h

# Check internet connection
ping ollama.com
```

---

### Slow Performance

**Problem:** Autocomplete/chat is slow

**Solutions:**

1. **Use smaller model:**
```bash
ollama pull qwen2.5-coder:1.5b
```

2. **Reduce context:**
```json
{
  "completionOptions": {
    "numCtx": 2048
  }
}
```

3. **Close other apps:**
```bash
# Free up RAM
```

4. **Enable GPU:**
```bash
# Check if GPU is being used
ollama ps
```

---

### Out of Memory

**Problem:** System runs out of RAM

**Solutions:**

1. **Use smaller model:**
```bash
ollama rm qwen2.5-coder:7b
ollama pull qwen2.5-coder:1.5b
```

2. **Reduce context window:**
```json
{
  "completionOptions": {
    "numCtx": 1024
  }
}
```

3. **Close other models:**
```bash
# Stop all models
ollama stop qwen2.5-coder:7b
```

---

### No Autocomplete Suggestions

**Problem:** Not seeing suggestions

**Solutions:**

1. **Check Ollama is running:**
```bash
ollama ps
```

2. **Verify model is loaded:**
```bash
ollama list
```

3. **Test model:**
```bash
ollama run qwen2.5-coder:1.5b "test"
```

4. **Check Continue config:**
```bash
cat ~/.continue/config.json
```

5. **Restart VS Code**

---

## üìä Performance Benchmarks

### Autocomplete Speed

| Model | Latency | Tokens/sec | RAM Usage |
|-------|---------|------------|-----------|
| Qwen 1.5B | 50-150ms | 100-200 | 2-3GB |
| Qwen 3B | 100-250ms | 50-100 | 4-5GB |
| Qwen 7B | 150-400ms | 30-60 | 8-10GB |

### Chat/Edit Speed

| Model | First Token | Tokens/sec | RAM Usage |
|-------|-------------|------------|-----------|
| Qwen 7B | 200-500ms | 30-60 | 8-10GB |
| Qwen 14B | 400-800ms | 20-40 | 16-20GB |
| Qwen 32B | 800-1500ms | 10-20 | 32-40GB |

**Note:** Performance varies by hardware (CPU/GPU/RAM)

---

## üéì Best Practices

### 1. Start Small
```bash
# Begin with smallest model
ollama pull qwen2.5-coder:1.5b

# Test it for a day
# Upgrade if needed
```

### 2. Monitor Performance
```bash
# Check resource usage
ollama ps

# Watch system resources
htop  # or Activity Monitor on macOS
```

### 3. Optimize Settings
```json
{
  "completionOptions": {
    "temperature": 0.2,  // Start here
    "numCtx": 2048,      // Increase if needed
    "numPredict": 256    // Adjust based on use
  }
}
```

### 4. Keep Models Updated
```bash
# Check for updates monthly
ollama pull qwen2.5-coder:1.5b
ollama pull qwen2.5-coder:7b
```

### 5. Clean Up Unused Models
```bash
# Remove models you don't use
ollama list
ollama rm model-name
```

---

## üöÄ Advanced Setup

### Multiple Models for Different Tasks

```json
{
  "models": [
    {
      "title": "Qwen 7B - General",
      "provider": "ollama",
      "model": "qwen2.5-coder:7b",
      "roles": ["chat"]
    },
    {
      "title": "Qwen 14B - Complex",
      "provider": "ollama",
      "model": "qwen2.5-coder:14b",
      "roles": ["edit"]
    },
    {
      "title": "Qwen 1.5B - Fast",
      "provider": "ollama",
      "model": "qwen2.5-coder:1.5b",
      "roles": ["autocomplete"]
    }
  ]
}
```

### Custom Model Parameters

```json
{
  "title": "Qwen Custom",
  "provider": "ollama",
  "model": "qwen2.5-coder:7b",
  "completionOptions": {
    "temperature": 0.3,
    "topP": 0.95,
    "topK": 40,
    "numPredict": 2048,
    "numCtx": 8192,
    "repeatPenalty": 1.1,
    "presencePenalty": 0.0,
    "frequencyPenalty": 0.0
  }
}
```

---

## üìö Resources

- **Ollama:** https://ollama.com
- **Ollama Models:** https://ollama.com/library
- **Continue Docs:** https://docs.continue.dev
- **Qwen Models:** https://huggingface.co/Qwen
- **Community:** https://discord.gg/continue

---

## ‚úÖ Success Checklist

- [ ] Ollama installed and running
- [ ] At least one model downloaded
- [ ] Continue configured with free models
- [ ] Autocomplete working
- [ ] Chat working
- [ ] Performance acceptable
- [ ] No cost incurred

---

## üéâ You're All Set!

You now have a **completely free** AI coding assistant that:
- ‚úÖ Runs 100% locally
- ‚úÖ Costs $0/month
- ‚úÖ Has no rate limits
- ‚úÖ Protects your privacy
- ‚úÖ Works offline
- ‚úÖ Provides intelligent code completion
- ‚úÖ Helps with chat and editing

**Enjoy coding with your free AI assistant!** üöÄ

---

**Last Updated:** October 15, 2025  
**Maintained by:** Mohamed + AI Team  
**Next Review:** November 15, 2025
