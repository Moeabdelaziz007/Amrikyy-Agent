# ðŸš€ Intake Analyzer - Deployment Guide

## Overview

The Intake Analyzer is now ready for deployment. This guide covers installation, configuration, testing, and monitoring.

## Prerequisites

### 1. Install Dependencies

```bash
cd backend
npm install @google/generative-ai node-cron
```

### 2. Environment Variables

Add to `.env`:

```bash
# Gemini API
GEMINI_API_KEY=your_gemini_api_key_here

# Supabase (should already exist)
SUPABASE_URL=your_supabase_url
SUPABASE_KEY=your_supabase_key
```

### 3. Database Schema

Create the `leads` table if it doesn't exist:

```sql
CREATE TABLE IF NOT EXISTS leads (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  user_id TEXT NOT NULL,
  message_id UUID REFERENCES messages(id),
  destination TEXT,
  origin TEXT,
  departure_date DATE,
  return_date DATE,
  budget NUMERIC,
  travelers INTEGER DEFAULT 1,
  preferences TEXT[],
  urgency TEXT CHECK (urgency IN ('low', 'medium', 'high')),
  trip_duration INTEGER,
  confidence_score INTEGER,
  raw_message TEXT,
  extracted_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  status TEXT DEFAULT 'new',
  processing_time_ms INTEGER,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Add indexes
CREATE INDEX idx_leads_user_id ON leads(user_id);
CREATE INDEX idx_leads_status ON leads(status);
CREATE INDEX idx_leads_created_at ON leads(created_at DESC);
CREATE INDEX idx_leads_destination ON leads(destination);

-- Add columns to messages table
ALTER TABLE messages 
ADD COLUMN IF NOT EXISTS processed BOOLEAN DEFAULT FALSE,
ADD COLUMN IF NOT EXISTS processed_at TIMESTAMP WITH TIME ZONE,
ADD COLUMN IF NOT EXISTS lead_id UUID REFERENCES leads(id),
ADD COLUMN IF NOT EXISTS processing_failed BOOLEAN DEFAULT FALSE,
ADD COLUMN IF NOT EXISTS processing_error TEXT,
ADD COLUMN IF NOT EXISTS processing_failed_at TIMESTAMP WITH TIME ZONE;

-- Add indexes
CREATE INDEX idx_messages_processed ON messages(processed);
CREATE INDEX idx_messages_processing_failed ON messages(processing_failed);
```

## Integration with Server

### Step 1: Register Routes

Add to `backend/server.js`:

```javascript
// Import automation routes
const automationRoutes = require('./routes/automation');

// Register routes (after other routes)
app.use('/api/automation', automationRoutes);
```

### Step 2: Start Cron Job

Add to `backend/server.js` (after server starts):

```javascript
// Import intake analyzer job
const intakeAnalyzerJob = require('./jobs/intakeAnalyzerJob');

// Start the job after server is running
const server = app.listen(PORT, () => {
  console.log(`Server running on port ${PORT}`);
  
  // Start Intake Analyzer cron job
  intakeAnalyzerJob.start();
});

// Graceful shutdown
process.on('SIGTERM', () => {
  console.log('SIGTERM received, shutting down gracefully');
  intakeAnalyzerJob.stop();
  server.close(() => {
    console.log('Server closed');
    process.exit(0);
  });
});
```

## Testing

### Manual Test

```bash
# Run test script
node scripts/test-intake-analyzer.js
```

Expected output:
```
ðŸ§ª Testing Intake Analyzer
================================================================================

ðŸ“ Test 1/5
Input: "Ø£Ø±ÙŠØ¯ Ø§Ù„Ø³ÙØ± Ø¥Ù„Ù‰ ØªØ±ÙƒÙŠØ§ Ù„Ù…Ø¯Ø© 7 Ø£ÙŠØ§Ù… Ù…Ø¹ Ø¹Ø§Ø¦Ù„ØªÙŠ (4 Ø£Ø´Ø®Ø§Øµ) Ø¨Ù…ÙŠØ²Ø§Ù†ÙŠØ© 5000 Ø¯ÙˆÙ„Ø§Ø±"
--------------------------------------------------------------------------------
âœ… Processed in 1234ms
Confidence: 95%

Extracted Data:
{
  "destination": "Turkey",
  "origin": null,
  "departureDate": null,
  "returnDate": null,
  "budget": 5000,
  "travelers": 4,
  "preferences": ["family-friendly"],
  "urgency": "medium",
  "tripDuration": 7
}

âœ… Test PASSED

...

ðŸ“Š Test Results:
   Total: 5
   Passed: 5 âœ…
   Failed: 0 âŒ
   Success Rate: 100%
```

### API Testing

#### 1. Process Single Message

```bash
curl -X POST http://localhost:5000/api/automation/process-message \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -d '{
    "messageId": "msg_123"
  }'
```

Expected response:
```json
{
  "success": true,
  "lead": {
    "id": "lead_456",
    "destination": "Turkey",
    "budget": 5000,
    ...
  },
  "extraction": {
    "destination": "Turkey",
    "travelers": 4,
    ...
  },
  "confidence": 95,
  "processingTime": 1234
}
```

#### 2. Process Batch

```bash
curl -X POST http://localhost:5000/api/automation/process-batch \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -d '{"limit": 10}'
```

Expected response:
```json
{
  "success": true,
  "processed": 10,
  "successful": 9,
  "failed": 1,
  "totalTime": 12340,
  "avgTime": 1234
}
```

#### 3. Get Statistics

```bash
curl http://localhost:5000/api/automation/statistics \
  -H "Authorization: Bearer YOUR_TOKEN"
```

Expected response:
```json
{
  "success": true,
  "intake": {
    "totalMessages": 100,
    "processedMessages": 90,
    "failedMessages": 5,
    "pendingMessages": 5,
    "totalLeads": 90,
    "successRate": 90,
    "avgConfidenceScore": 87,
    "avgProcessingTime": 1234
  },
  "job": {
    "totalRuns": 50,
    "totalProcessed": 90,
    "totalSuccessful": 85,
    "totalFailed": 5,
    "successRate": 94,
    "lastRun": "2025-10-17T12:00:00Z"
  }
}
```

#### 4. Health Check

```bash
curl http://localhost:5000/api/automation/health
```

Expected response:
```json
{
  "success": true,
  "services": {
    "gemini": {
      "healthy": true,
      "sdk": true
    },
    "intakeAnalyzer": {
      "healthy": true,
      "stats": {...}
    }
  }
}
```

## Monitoring

### 1. Check Logs

```bash
# View real-time logs
pm2 logs backend

# Filter for Intake Analyzer
pm2 logs backend | grep "Intake Analyzer"
```

### 2. Monitor Statistics

Create a monitoring dashboard endpoint:

```javascript
// Add to backend/routes/automation.js
router.get('/dashboard', authenticateToken, async (req, res) => {
  const stats = await IntakeAnalyzer.getStatistics();
  const jobStats = intakeAnalyzerJob.getStats();
  
  res.json({
    automation: {
      currentLevel: '50%',
      target: '85%',
      timeSaved: '5-15 hours/day'
    },
    intake: stats,
    job: jobStats,
    performance: {
      avgProcessingTime: stats.avgProcessingTime + 'ms',
      successRate: stats.successRate + '%',
      confidence: stats.avgConfidenceScore + '%'
    }
  });
});
```

### 3. Set Up Alerts

```javascript
// Add to backend/jobs/intakeAnalyzerJob.js
async function runJob() {
  const result = await IntakeAnalyzer.processUnprocessedMessages(10);
  
  // Alert if success rate drops below 80%
  if (result.successful / result.processed < 0.8) {
    logger.error('âš ï¸ Intake Analyzer success rate below 80%', {
      successRate: (result.successful / result.processed * 100).toFixed(2) + '%'
    });
    
    // Send alert (Telegram, email, etc.)
    await sendAlert('Intake Analyzer performance degraded');
  }
  
  return result;
}
```

## Performance Optimization

### 1. Caching

Enable caching for similar messages:

```javascript
// Add to backend/services/automation/IntakeAnalyzer.js
const NodeCache = require('node-cache');
const cache = new NodeCache({ stdTTL: 3600 }); // 1 hour

static async processMessage(message) {
  // Check cache
  const cacheKey = `extraction:${message.text}`;
  const cached = cache.get(cacheKey);
  
  if (cached) {
    logger.info('Using cached extraction');
    return cached;
  }
  
  // Process normally
  const result = await GeminiCLI.extractSDK(message.text, prompt);
  
  // Cache result
  cache.set(cacheKey, result);
  
  return result;
}
```

### 2. Rate Limiting

Implement rate limiting for Gemini API:

```javascript
const Bottleneck = require('bottleneck');

const limiter = new Bottleneck({
  maxConcurrent: 5,  // Max 5 concurrent requests
  minTime: 200       // Min 200ms between requests
});

class GeminiCLI {
  async extractSDK(text, prompt) {
    return limiter.schedule(async () => {
      // Actual Gemini call
      const result = await this.model.generateContent(fullPrompt);
      return result;
    });
  }
}
```

### 3. Batch Processing

Increase batch size during off-peak hours:

```javascript
// Modify backend/jobs/intakeAnalyzerJob.js
function start() {
  // Peak hours (9 AM - 6 PM): Process 10 messages/minute
  cron.schedule('* 9-18 * * *', async () => {
    await IntakeAnalyzer.processUnprocessedMessages(10);
  });
  
  // Off-peak hours: Process 50 messages/minute
  cron.schedule('* 0-8,19-23 * * *', async () => {
    await IntakeAnalyzer.processUnprocessedMessages(50);
  });
}
```

## Troubleshooting

### Issue 1: Low Confidence Scores

**Symptom:** Average confidence < 70%

**Solutions:**
1. Improve prompt engineering
2. Add more context to extraction prompt
3. Train on more examples
4. Adjust confidence calculation weights

### Issue 2: High Processing Time

**Symptom:** Average processing time > 3000ms

**Solutions:**
1. Enable caching
2. Reduce Gemini model temperature
3. Optimize prompt length
4. Use parallel processing

### Issue 3: Extraction Errors

**Symptom:** Failed extractions > 10%

**Solutions:**
1. Add retry logic with exponential backoff
2. Improve error handling
3. Add fallback to simpler extraction
4. Log failed messages for analysis

### Issue 4: Cron Job Not Running

**Symptom:** No messages being processed automatically

**Solutions:**
1. Check if job is started in server.js
2. Verify cron schedule syntax
3. Check server logs for errors
4. Manually trigger job via API

## Success Metrics

### Target KPIs (Week 1)
- âœ… Processing time: <2000ms average
- âœ… Success rate: >90%
- âœ… Confidence score: >80% average
- âœ… Messages processed: 100+ per day
- âœ… Time saved: 5-10 hours/day

### Target KPIs (Week 4)
- âœ… Processing time: <1000ms average
- âœ… Success rate: >95%
- âœ… Confidence score: >85% average
- âœ… Messages processed: 500+ per day
- âœ… Time saved: 10-15 hours/day

## Next Steps

1. âœ… Deploy to staging environment
2. âœ… Run tests with real data
3. âœ… Monitor for 24 hours
4. âœ… Optimize based on metrics
5. âœ… Deploy to production
6. âœ… Enable cron job
7. âœ… Monitor and iterate

## Support

For issues or questions:
- Check logs: `pm2 logs backend`
- View statistics: `GET /api/automation/statistics`
- Manual trigger: `POST /api/automation/run-job`
- Health check: `GET /api/automation/health`

---

**Status:** Ready for Deployment  
**Priority:** P0 - Critical  
**Expected Impact:** 5-15 hours/day saved  
**Automation Increase:** 28% â†’ 50%+
